{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cofactor\n",
    "\n",
    "Liang's extension of Alternating Least Squares Algorithm. [Factorization Meets the Item Embedding: Regularizing Matrix Factorization with Item Co-occurrence](https://dl.acm.org/doi/10.1145/2959100.2959182)\n",
    "\n",
    "It co-factorizes both user-item interaction matrix and SPPMI matrix(kind of item-item co-occurence matrix) with shared item matrix. It claims that two different matrix reveals different information, thus exploiting both matrix will be helpful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from buffalo.algo.cfr import CFR\n",
    "from buffalo.algo.options import CFROption\n",
    "from buffalo.data.stream import StreamOptions\n",
    "from buffalo.misc import aux\n",
    "from buffalo.misc import log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'evaluation_on_learning': True,\n",
       " 'compute_loss_on_training': True,\n",
       " 'early_stopping_rounds': 0,\n",
       " 'save_best': False,\n",
       " 'evaluation_period': 1,\n",
       " 'save_period': 10,\n",
       " 'random_seed': 0,\n",
       " 'validation': {},\n",
       " 'save_factors': False,\n",
       " 'd': 20,\n",
       " 'num_iters': 10,\n",
       " 'num_workers': 1,\n",
       " 'num_cg_max_iters': 3,\n",
       " 'cg_tolerance': 1e-10,\n",
       " 'eps': 1e-10,\n",
       " 'reg_u': 0.1,\n",
       " 'reg_i': 0.1,\n",
       " 'reg_c': 0.1,\n",
       " 'alpha': 8.0,\n",
       " 'l': 1.0,\n",
       " 'optimizer': 'manual_cg',\n",
       " 'model_path': '',\n",
       " 'data_opt': {}}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt = CFROption().get_default_option() # initialize default Cofactor option\n",
    "opt                                    # Check buffalo/algo/options.py to see further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_opt = StreamOptions().get_default_option()\n",
    "data_opt.data.sppmi = {\"windows\": 5, \"k\": 10}\n",
    "data_opt.input.main = 'data/ml-1m/stream'\n",
    "data_opt.input.uid = 'data/ml-1m/uid'\n",
    "data_opt.input.iid = 'data/ml-1m/iid'\n",
    "data_opt.data.value_prepro = aux.Option({'name': 'OneBased'})\n",
    "data_opt.data.path = './3-cfr.h5py'\n",
    "data_opt.data.internal_data_type = 'matrix'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO    ] 2019-10-04 10:40:41 [stream.py:278] Create database from stream data\n",
      "[INFO    ] 2019-10-04 10:40:41 [stream.py:101] gathering itemids from data/ml-1m/stream...\n",
      "[INFO    ] 2019-10-04 10:40:41 [stream.py:125] Found 3706 unique itemids\n",
      "[INFO    ] 2019-10-04 10:40:41 [stream.py:287] Creating working data...\n",
      "[INFO    ] 2019-10-04 10:40:49 [stream.py:295] Building data part...\n",
      "[INFO    ] 2019-10-04 10:40:49 [base.py:362] Building compressed triplets for rowwise...\n",
      "[INFO    ] 2019-10-04 10:40:49 [base.py:363] Preprocessing...\n",
      "[INFO    ] 2019-10-04 10:40:49 [base.py:366] In-memory Compressing ...\n",
      "[INFO    ] 2019-10-04 10:40:50 [base.py:249] Load triplet files. Total job files: 7\n",
      "[INFO    ] 2019-10-04 10:40:50 [base.py:396] Finished\n",
      "[INFO    ] 2019-10-04 10:40:50 [base.py:362] Building compressed triplets for colwise...\n",
      "[INFO    ] 2019-10-04 10:40:50 [base.py:363] Preprocessing...\n",
      "[INFO    ] 2019-10-04 10:40:50 [base.py:366] In-memory Compressing ...\n",
      "[INFO    ] 2019-10-04 10:40:51 [base.py:249] Load triplet files. Total job files: 7\n",
      "[INFO    ] 2019-10-04 10:40:51 [base.py:396] Finished\n",
      "[INFO    ] 2019-10-04 10:40:51 [stream.py:166] build sppmi (shift k: 10)\n",
      "[INFO    ] 2019-10-04 10:41:01 [stream.py:177] convert from /tmp/tmp8ztbh60r to /tmp/tmpt4rxnat9\n",
      "[INFO    ] 2019-10-04 10:41:01 [stream.py:180] sppmi nnz: 350626\n",
      "[INFO    ] 2019-10-04 10:41:01 [stream.py:184] Disk-based Compressing...\n",
      "[INFO    ] 2019-10-04 10:41:01 [base.py:294] Dividing into 12 chunks...\n",
      "[INFO    ] 2019-10-04 10:41:01 [base.py:304] Total job files: 12\n",
      "[PROGRESS] 100.00% 1.1/1.1secs 11.06it/s\n",
      "[INFO    ] 2019-10-04 10:41:03 [stream.py:312] DB built on ./3-cfr.h5py\n",
      "[INFO    ] 2019-10-04 10:41:03 [cfr.py:62] CFR ({\n",
      "  \"evaluation_on_learning\": true,\n",
      "  \"compute_loss_on_training\": true,\n",
      "  \"early_stopping_rounds\": 0,\n",
      "  \"save_best\": false,\n",
      "  \"evaluation_period\": 1,\n",
      "  \"save_period\": 10,\n",
      "  \"random_seed\": 0,\n",
      "  \"validation\": {},\n",
      "  \"save_factors\": false,\n",
      "  \"d\": 20,\n",
      "  \"num_iters\": 10,\n",
      "  \"num_workers\": 1,\n",
      "  \"num_cg_max_iters\": 3,\n",
      "  \"cg_tolerance\": 1e-10,\n",
      "  \"eps\": 1e-10,\n",
      "  \"reg_u\": 0.1,\n",
      "  \"reg_i\": 0.1,\n",
      "  \"reg_c\": 0.1,\n",
      "  \"alpha\": 8.0,\n",
      "  \"l\": 1.0,\n",
      "  \"optimizer\": \"manual_cg\",\n",
      "  \"model_path\": \"\",\n",
      "  \"data_opt\": {}\n",
      "})\n",
      "[INFO    ] 2019-10-04 10:41:03 [cfr.py:64] Stream Header(6040, 3706, 999709) Validation(500 samples)\n"
     ]
    }
   ],
   "source": [
    "cofactor = CFR(opt, data_opt=data_opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cofactor.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO    ] 2019-10-04 10:41:03 [buffered_data.py:71] Set data buffer size as 67108864(minimum required batch size is 245).\n",
      "[INFO    ] 2019-10-04 10:41:03 [cfr.py:207] Iteration 1: Loss 0.000 Elapsed 0.518 secs\n",
      "[INFO    ] 2019-10-04 10:41:04 [cfr.py:207] Iteration 2: Loss 0.000 Elapsed 0.477 secs\n",
      "[INFO    ] 2019-10-04 10:41:04 [cfr.py:207] Iteration 3: Loss 0.000 Elapsed 0.486 secs\n",
      "[INFO    ] 2019-10-04 10:41:05 [cfr.py:207] Iteration 4: Loss 0.000 Elapsed 0.490 secs\n",
      "[INFO    ] 2019-10-04 10:41:05 [cfr.py:207] Iteration 5: Loss 0.000 Elapsed 0.489 secs\n",
      "[INFO    ] 2019-10-04 10:41:06 [cfr.py:207] Iteration 6: Loss 0.000 Elapsed 0.415 secs\n",
      "[INFO    ] 2019-10-04 10:41:06 [cfr.py:207] Iteration 7: Loss 0.000 Elapsed 0.267 secs\n",
      "[INFO    ] 2019-10-04 10:41:06 [cfr.py:207] Iteration 8: Loss 0.000 Elapsed 0.181 secs\n",
      "[INFO    ] 2019-10-04 10:41:06 [cfr.py:207] Iteration 9: Loss 0.000 Elapsed 0.320 secs\n",
      "[INFO    ] 2019-10-04 10:41:07 [cfr.py:207] Iteration 10: Loss 0.000 Elapsed 0.380 secs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'train_loss': 0.0}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cofactor.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recommendation for users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for user 61, recommendations are  \n",
      "items ['Patriot,_The_(2000)', 'Perfect_Storm,_The_(2000)', 'Scary_Movie_(2000)'].\n",
      "\n",
      "for user 62, recommendations are  \n",
      "items ['Rear_Window_(1954)', 'Witness_(1985)', 'Chinatown_(1974)'].\n",
      "\n",
      "for user 63, recommendations are  \n",
      "items ['Austin_Powers:_The_Spy_Who_Shagged_Me_(1999)', 'Blair_Witch_Project,_The_(1999)', 'American_Pie_(1999)'].\n",
      "\n",
      "for user 64, recommendations are  \n",
      "items ['Jurassic_Park_(1993)', 'Terminator_2:_Judgment_Day_(1991)', 'American_Beauty_(1999)'].\n",
      "\n",
      "for user 65, recommendations are  \n",
      "items ['Braveheart_(1995)', 'Saving_Private_Ryan_(1998)', 'Jurassic_Park_(1993)'].\n",
      "\n",
      "for user 66, recommendations are  \n",
      "items ['Braveheart_(1995)', 'American_Beauty_(1999)', 'Airplane!_(1980)'].\n",
      "\n",
      "for user 67, recommendations are  \n",
      "items ['Bridge_on_the_River_Kwai,_The_(1957)', 'To_Kill_a_Mockingbird_(1962)', 'Graduate,_The_(1967)'].\n",
      "\n",
      "for user 68, recommendations are  \n",
      "items ['Shakespeare_in_Love_(1998)', 'Groundhog_Day_(1993)', 'Toy_Story_2_(1999)'].\n",
      "\n",
      "for user 69, recommendations are  \n",
      "items ['Good_Will_Hunting_(1997)', 'Dead_Man_Walking_(1995)', 'Shawshank_Redemption,_The_(1994)'].\n",
      "\n"
     ]
    }
   ],
   "source": [
    "uids = [str(x) for x in range(61, 70)]\n",
    "recommendation_result = cofactor.topk_recommendation(uids, topk=3)\n",
    "for uid, iids in recommendation_result.items():\n",
    "    print(f\"for user {uid}, recommendations are \", f\"\\nitems {iids}.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recommendation for users in given pools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for user 1, recommendations are  \n",
      "items ['Shanghai_Noon_(2000)', 'Frequency_(2000)', 'Remember_the_Titans_(2000)'].\n",
      "\n",
      "for user 2, recommendations are  \n",
      "items ['Remember_the_Titans_(2000)', 'Shanghai_Noon_(2000)', 'Frequency_(2000)'].\n",
      "\n",
      "for user 3, recommendations are  \n",
      "items ['Shanghai_Noon_(2000)', '28_Days_(2000)', 'Frequency_(2000)'].\n",
      "\n",
      "for user 4, recommendations are  \n",
      "items ['Shanghai_Noon_(2000)', 'Final_Destination_(2000)', 'Frequency_(2000)'].\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pool = ['Rules_of_Engagement_(2000)', \n",
    "        'Remember_the_Titans_(2000)', \n",
    "        'Skulls,_The_(2000)', \n",
    "        '28_Days_(2000)', \n",
    "        'Frequency_(2000)', \n",
    "        'Gone_in_60_Seconds_(2000)', \n",
    "        'What_Lies_Beneath_(2000)', \n",
    "        'Reindeer_Games_(2000)', \n",
    "        'Final_Destination_(2000)', \n",
    "        'Shanghai_Noon_(2000)']\n",
    "uids = [str(x) for x in range(5)]\n",
    "recommendation_result = cofactor.topk_recommendation(uids, topk=3, pool=pool)\n",
    "for uid, iids in recommendation_result.items():\n",
    "    print(f\"for user {uid}, recommendations are \", f\"\\nitems {iids}.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find Most similar items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similar movies to Toy_Story_2_(1999) in similar items\n",
      "[(\"Bug's_Life,_A_(1998)\", 0.93695074), ('Toy_Story_(1995)', 0.91278535), ('Babe_(1995)', 0.8598581), ('Shakespeare_in_Love_(1998)', 0.84673494), ('Being_John_Malkovich_(1999)', 0.83271587), ('Election_(1999)', 0.8022457), ('American_Beauty_(1999)', 0.788048), ('South_Park:_Bigger,_Longer_and_Uncut_(1999)', 0.77576375), ('Groundhog_Day_(1993)', 0.7618997), ('Aladdin_(1992)', 0.7471918)]\n",
      "01. 0.937 Bug's_Life,_A_(1998)\n",
      "02. 0.913 Toy_Story_(1995)\n",
      "03. 0.860 Babe_(1995)\n",
      "04. 0.847 Shakespeare_in_Love_(1998)\n",
      "05. 0.833 Being_John_Malkovich_(1999)\n",
      "06. 0.802 Election_(1999)\n",
      "07. 0.788 American_Beauty_(1999)\n",
      "08. 0.776 South_Park:_Bigger,_Longer_and_Uncut_(1999)\n",
      "09. 0.762 Groundhog_Day_(1993)\n",
      "10. 0.747 Aladdin_(1992)\n"
     ]
    }
   ],
   "source": [
    "print('Similar movies to Toy_Story_2_(1999) in similar items')\n",
    "similar_items = cofactor.most_similar('Toy_Story_2_(1999)', 10)\n",
    "print(similar_items)\n",
    "for rank, (movie_name, score) in enumerate(similar_items):\n",
    "    print(f'{rank + 1:02d}. {score:.3f} {movie_name}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find Most similar items given pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01. 0.385 Shanghai_Noon_(2000)\n",
      "02. 0.379 28_Days_(2000)\n",
      "03. 0.364 Frequency_(2000)\n",
      "04. 0.297 Gone_in_60_Seconds_(2000)\n",
      "05. 0.224 Final_Destination_(2000)\n",
      "06. 0.195 What_Lies_Beneath_(2000)\n"
     ]
    }
   ],
   "source": [
    "pool = ['Rules_of_Engagement_(2000)', \n",
    "        'Remember_the_Titans_(2000)', \n",
    "        'Skulls,_The_(2000)', \n",
    "        '28_Days_(2000)', \n",
    "        'Frequency_(2000)', \n",
    "        'Gone_in_60_Seconds_(2000)', \n",
    "        'What_Lies_Beneath_(2000)', \n",
    "        'Reindeer_Games_(2000)', \n",
    "        'Final_Destination_(2000)', \n",
    "        'Shanghai_Noon_(2000)']\n",
    "similar_items = cofactor.most_similar('Toy_Story_2_(1999)', 5, pool=pool)\n",
    "for rank, (movie_name, score) in enumerate(similar_items):\n",
    "    print(f'{rank + 1:02d}. {score:.3f} {movie_name}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
